# Create the complete WhatsApp Chat Analyzer app including all requested features

import streamlit as st
import pandas as pd
import re
from urlextract import URLExtract
from textblob import TextBlob
from textblob.sentiments import PatternAnalyzer
from collections import Counter
import matplotlib.pyplot as plt
import seaborn as sns
import emoji
from wordcloud import WordCloud
from datetime import datetime
import numpy as np

st.set_page_config(page_title="WhatsApp Chat Analyzer", layout="wide")
st.title("ğŸ“± WhatsApp Chat Analyzer")

uploaded_file = st.sidebar.file_uploader("Upload your WhatsApp chat file (.txt)", type=["txt"])

def get_sentiment(text):
    # You could use langdetect or similar to detect language
    # For now, only analyze if Spanish
    # Otherwise, return None or a default value
    return TextBlob(text, analyzer=PatternAnalyzer()).sentiment[0]

if uploaded_file:
    chat_data = uploaded_file.read().decode("utf-8")

    pattern = r'^(\d{1,2}/\d{1,2}/\d{2,4}), (\d{1,2}:\d{2}) - ([^:]+): (.*)$'
    lines = chat_data.splitlines()

    data = []
    for line in lines:
        match = re.match(pattern, line)
        if match:
            date, time, user, message = match.groups()
            try:
                datetime_obj = datetime.strptime(f"{date} {time}", "%d/%m/%Y %H:%M")
            except ValueError:
                try:
                    datetime_obj = datetime.strptime(f"{date} {time}", "%d/%m/%y %H:%M")
                except:
                    continue
            data.append({
                'datetime': datetime_obj,
                'user': user.strip(),
                'message': message.strip()
            })

    df = pd.DataFrame(data)
    if df.empty:
        st.warning("No messages parsed. Please check your file format.")
        st.stop()

    df['date'] = df['datetime'].dt.date
    df['time'] = df['datetime'].dt.time
    df['hour'] = df['datetime'].dt.hour
    df['day'] = df['datetime'].dt.date
    df['month'] = df['datetime'].dt.to_period('M')
    df['weekday'] = df['datetime'].dt.day_name()

    # Extract URLs
    extractor = URLExtract()
    df['num_words'] = df['message'].apply(lambda x: len(x.split()))
    df['has_media'] = df['message'].str.contains('edia')
    df['num_links'] = df['message'].apply(lambda x: len(extractor.find_urls(x)))

    # Sentiment
    df['sentiment'] = df['message'].apply(get_sentiment)

    tab1, tab2, tab3, tab4, tab5, tab6 = st.tabs([
        "ğŸ“Š EstadÃ­sticas", "ğŸ“ˆ Actividad", "ğŸ—£ï¸ ParticipaciÃ³n", "ğŸ˜‚ Emojis y Wordcloud", "ğŸ” Avanzado", "ğŸ§  NLP"
    ])


    with tab1:
        st.header("ğŸ“Š EstadÃ­sticas")
        col1, col2, col3, col4 = st.columns(4)
        col1.metric("Total mensajes", len(df))
        col2.metric("Total palabras", df['num_words'].sum())
        col3.metric("Archivos multimedia", df['has_media'].sum())
        col4.metric("Enlaces compartidos", df['num_links'].sum())
        #top days, hours, users
        st.subheader("Top 5 dÃ­as con mÃ¡s mensajes")
        top_days = df['day'].value_counts().head(5)
        st.write(top_days)
        st.subheader("Top 5 horas con mÃ¡s mensajes")
        top_hours = df['hour'].value_counts().head(5)
        st.write(top_hours)
        st.subheader("Top 5 usuarios")
        top_users = df['user'].value_counts().head(5)
        st.write(top_users)
        # Rachas de mÃ¡s dÃ­as consecutivos con y sin mensajes (y quÃ© fechas son)
        st.subheader("Rachas de dÃ­as consecutivos sin mensajes")
        # Crear un rango completo de fechas
        all_days = pd.date_range(df['day'].min(), df['day'].max())
        msg_per_day = df.groupby('day').size().reindex(all_days, fill_value=0)
        # Encontrar rachas de dÃ­as sin mensajes
        no_msg_streaks = []
        current_streak = []
        for date, count in msg_per_day.items():
            if count == 0:
                current_streak.append(date)
            else:
                if len(current_streak) > 0:
                    no_msg_streaks.append(list(current_streak))
                    current_streak = []
        if len(current_streak) > 0:
            no_msg_streaks.append(list(current_streak))
        # Mostrar las rachas mÃ¡s largas
        no_msg_streaks = sorted(no_msg_streaks, key=len, reverse=True)
        if no_msg_streaks and len(no_msg_streaks[0]) > 0:
            st.write(f"Mayor racha sin mensajes: {len(no_msg_streaks[0])} dÃ­as, desde {no_msg_streaks[0][0].date()} hasta {no_msg_streaks[0][-1].date()}")
        else:
            st.write("No hubo dÃ­as consecutivos sin mensajes.")

        st.subheader("Rachas de dÃ­as consecutivos con mensajes")
        msg_streaks = []
        current_streak = []
        for date, count in msg_per_day.items():
            if count > 0:
                current_streak.append(date)
            else:
                if len(current_streak) > 0:
                    msg_streaks.append(list(current_streak))
                    current_streak = []
        if len(current_streak) > 0:
            msg_streaks.append(list(current_streak))
        msg_streaks = sorted(msg_streaks, key=len, reverse=True)
        if msg_streaks and len(msg_streaks[0]) > 0:
            st.write(f"Mayor racha con mensajes: {len(msg_streaks[0])} dÃ­as, desde {msg_streaks[0][0].date()} hasta {msg_streaks[0][-1].date()}")
        else:
            st.write("No hubo dÃ­as consecutivos con mensajes.")


        # Longitud media de mensajes
        st.subheader("Longitud media de mensajes")
        df['message_length'] = df['message'].apply(lambda x: len(x.split()))
        avg_length = df['message_length'].mean()

        col_a, col_b = st.columns([1, 2])
        with col_a:
            st.write(f"Longitud media de mensajes: {avg_length:.2f} palabras")
            # Top 3 usuarios con mayor longitud media de mensajes
            st.write("Top 3 usuarios con mayor longitud media de mensajes:")
            avg_length_by_user = df.groupby('user')['message_length'].mean().sort_values(ascending=False).head(3)
            st.write(avg_length_by_user)

        with col_b:
            fig1, ax1 = plt.subplots()
            sns.histplot(df['message_length'], bins=30, ax=ax1)
            ax1.set_xlabel("Longitud del mensaje (palabras)")
            ax1.set_ylabel("Frecuencia")
            st.pyplot(fig1)

    with tab2:
        st.header("ğŸ“ˆ Actividad")
        st.subheader("Mensajes por hora")
        fig, ax = plt.subplots()
        sns.histplot(df['hour'], bins=24, ax=ax)
        ax.set_xlabel("Hora del dÃ­a")
        ax.set_ylabel("NÃºmero de mensajes")
        st.pyplot(fig)

        st.subheader("Mensajes por semana/mes")
        st.line_chart(df.groupby('month').size())

        st.subheader("Actividad diaria")
        st.line_chart(df.groupby('day').size())

        st.subheader("Sentimiento medio por dÃ­a")
        st.line_chart(df.groupby('day')['sentiment'].mean())

        st.subheader("Heatmap: Mensajes por hora y dÃ­a de la semana")
        pivot = pd.pivot_table(df, index='weekday', columns='hour', values='message', aggfunc='count').fillna(0)
        # Reorder weekdays
        ordered_days = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']
        pivot = pivot.reindex(ordered_days)
        fig2, ax2 = plt.subplots(figsize=(12, 4))
        sns.heatmap(pivot, cmap="YlGnBu", ax=ax2)
        ax2.set_xlabel("Hora del dÃ­a")
        ax2.set_ylabel("DÃ­a de la semana")
        st.pyplot(fig2)

        # EvoluciÃ³n del nÃºmero de mensajes en el tiempo (acumulado y rolling mean)
        st.subheader("EvoluciÃ³n acumulada de mensajes")
        cumulative_msgs = df.groupby('day').size().cumsum()
        st.line_chart(cumulative_msgs)

        st.subheader("Media mÃ³vil de mensajes (7 dÃ­as)")
        rolling_msgs = df.groupby('day').size().rolling(window=30, min_periods=1).mean()
        st.line_chart(rolling_msgs)


    with tab3:
        st.header("ğŸ—£ï¸ ParticipaciÃ³n")
        st.subheader("Mensajes por usuario")
        user_msg_count = df['user'].value_counts()
        st.bar_chart(user_msg_count)

        st.subheader("ParticipaciÃ³n (%)")
        fig3, ax3 = plt.subplots()
        user_msg_count.plot.pie(autopct='%1.1f%%', ax=ax3)
        ax3.set_ylabel("")
        st.pyplot(fig3)

        # Lineplot de mensajes por usuario en el tiempo (acumulado y rolling mean)
        st.subheader("EvoluciÃ³n acumulada de mensajes por usuario")
        user_cum_msgs = df.groupby(['day', 'user']).size().unstack(fill_value=0).cumsum()
        st.line_chart(user_cum_msgs)

        st.subheader("Media mÃ³vil de mensajes por usuario (7 dÃ­as)")
        user_rolling_msgs = df.groupby(['day', 'user']).size().unstack(fill_value=0).rolling(window=30, min_periods=1).mean()
        st.line_chart(user_rolling_msgs)

        


        

    with tab4:
        st.subheader("Palabras mÃ¡s comunes")
        all_words = ' '.join(df['message'].tolist())
        words = re.findall(r'\b\w+\b', all_words.lower())
        words = [word for word in words if len(word) > 3]
        words = [word for word in words if word not in ["multimedia", "media", "omitido", "enlace", "link", "null", "mensaje", "este", "eliminado", "elimino", "eliminÃ³", "omitted", "https", "status", "deleted"]]
        common_words = Counter(words).most_common(10)
        st.write(pd.DataFrame(common_words, columns=["Palabra", "Frecuencia"]))

        st.header("ğŸ˜‚ Emojis y Wordcloud")
        emoji_list = [c for c in all_words if c in emoji.EMOJI_DATA]
        emoji_freq = Counter(emoji_list).most_common(10)
        st.subheader("Emojis mÃ¡s usados")
        st.write(pd.DataFrame(emoji_freq, columns=["Emoji", "Frecuencia"]))

        st.subheader("Nube de palabras")
        wordcloud = WordCloud(width=800, height=400, background_color='white').generate(' '.join(words))
        fig_wc, ax_wc = plt.subplots(figsize=(10, 5))
        ax_wc.imshow(wordcloud, interpolation='bilinear')
        ax_wc.axis('off')
        st.pyplot(fig_wc)

    with tab5:
        st.header("ğŸ” Avanzado")
        st.subheader("Mensajes con enlaces")
        top_link_users = df[df['num_links'] > 0]['user'].value_counts()
        if not top_link_users.empty:
            st.write(f"Usuario(s) que mÃ¡s han enviado enlaces: {top_link_users.idxmax()} ({top_link_users.max()} enlaces)")
        else:
            st.write("No se encontraron mensajes con enlaces.")

        st.subheader("Mensajes con archivos multimedia")
        top_media_users = df[df['has_media']]['user'].value_counts()
        if not top_media_users.empty:
            st.write(f"Usuario(s) que mÃ¡s han enviado archivos multimedia: {top_media_users.idxmax()} ({top_media_users.max()} archivos)")
        else:
            st.write("No se encontraron mensajes con archivos multimedia.")

        st.subheader("QuiÃ©n responde a quiÃ©n (Ãºltimos 10 minutos)")
        # Ordena por datetime por si acaso
        df_sorted = df.sort_values('datetime').reset_index(drop=True)
        df_sorted['prev_user'] = df_sorted['user'].shift(1)
        df_sorted['prev_time'] = df_sorted['datetime'].shift(1)
        df_sorted['time_diff'] = (df_sorted['datetime'] - df_sorted['prev_time']).dt.total_seconds() / 60  # minutos

        # Solo cuenta si el usuario cambia y la diferencia es menor a 10 minutos
        mask = (df_sorted['user'] != df_sorted['prev_user']) & (df_sorted['time_diff'] <= 10)
        reply_pairs = df_sorted.loc[mask, ['prev_user', 'user']]

        # Cuenta las respuestas
        reply_counts = reply_pairs.groupby(['prev_user', 'user']).size().reset_index(name='responses')
        reply_matrix = reply_counts.pivot(index='prev_user', columns='user', values='responses').fillna(0).astype(int)

        st.dataframe(reply_matrix)

        # Heatmap de respuestas absolutas
        st.subheader("Heatmap de respuestas (nÃºmero absoluto)")
        fig_abs, ax_abs = plt.subplots(figsize=(6, 4))
        sns.heatmap(reply_matrix, annot=True, fmt="d", cmap="Blues", ax=ax_abs)
        ax_abs.set_xlabel("Responde")
        ax_abs.set_ylabel("Recibe")
        st.pyplot(fig_abs)

        # Heatmap de respuestas en porcentaje
        st.subheader("Heatmap de respuestas (%)")
        reply_matrix_pct = reply_matrix.div(reply_matrix.sum(axis=1), axis=0).fillna(0) * 100
        fig_pct, ax_pct = plt.subplots(figsize=(6, 4))
        sns.heatmap(reply_matrix_pct, annot=True, fmt=".1f", cmap="YlOrRd", ax=ax_pct)
        ax_pct.set_xlabel("Responde")
        ax_pct.set_ylabel("Recibe")
        st.pyplot(fig_pct)

        st.subheader("DistribuciÃ³n de sentimiento")
        fig4, ax4 = plt.subplots()
        sns.histplot(df['sentiment'], bins=20, ax=ax4)
        ax4.set_xlabel("Sentimiento (polarity)")
        st.pyplot(fig4)

        st.subheader("Menciones entre usuarios")

        # Obtener usuarios Ãºnicos (evitando mensajes del sistema)
        users = df['user'].dropna().apply(lambda x: x.split()[0] if isinstance(x, str) else x).unique().tolist()
        users_clean = [u for u in users if len(u.split()) < 5 and not re.match(r'^\d+$', u)]

        # Crear mapa de nombres en minÃºscula para matching
        user_lc_map = {u: u.lower() for u in users_clean}

        # Inicializar matriz de menciones
        mention_counts = pd.DataFrame(0, index=users_clean, columns=users_clean)

        # Recorrer mensajes y contar menciones
        for idx, row in df.iterrows():
            msg = str(row['message']).lower()
            sender = row['user']
            if sender not in users_clean:
                continue
            for target in users_clean:
                if target == sender:
                    continue
                pattern = r'\b' + re.escape(user_lc_map[target]) + r'\b'
                if re.search(pattern, msg):
                    mention_counts.loc[sender, target] += 1

        # Mostrar tabla
        st.dataframe(mention_counts)

        # Mostrar heatmap
        st.subheader("Heatmap de menciones")
        fig_mentions, ax_mentions = plt.subplots(figsize=(6, 4))
        sns.heatmap(mention_counts, annot=True, fmt="d", cmap="Greens", ax=ax_mentions)
        ax_mentions.set_xlabel("Mencionado")
        ax_mentions.set_ylabel("Quien menciona")
        st.pyplot(fig_mentions)



    
    with tab6:
        st.header("ğŸ§  AnÃ¡lisis NLP: Tono Emocional y Relaciones")

        import nltk
        from nltk.tokenize import word_tokenize
        from rapidfuzz import fuzz
        from collections import defaultdict
        import os

        nltk.download('punkt')

        keywords_path = "data/emotion_keywords.csv"

        if not os.path.exists(keywords_path):
            st.error(f"Keyword file not found: {keywords_path}")
            st.stop()

        try:
            keywords_df = pd.read_csv(keywords_path)
            keywords_df['tone'] = keywords_df['tone'].str.strip()
            keywords_df['word'] = keywords_df['word'].str.strip().str.lower()
        except Exception as e:
            st.error(f"Error loading keyword file: {e}")
            st.stop()

        # Diccionario: tone -> list of words
        tone_keywords = defaultdict(list)
        for _, row in keywords_df.iterrows():
            tone_keywords[row['tone']].append(row['word'])

        import re
        from collections import defaultdict
        from rapidfuzz import fuzz

        def simple_tokenize(text):
            return re.findall(r'\b\w{2,}\b', text.lower())

        def classify_tone_fuzzy(msg, threshold=85):
            tokens = simple_tokenize(msg)
            matches = defaultdict(int)
            for tone, words in tone_keywords.items():
                for word in words:
                    for token in tokens:
                        if fuzz.partial_ratio(token, word) >= threshold:
                            matches[tone] += 1
            return max(matches, key=matches.get) if matches else 'other'

        df['tone'] = df['message'].apply(classify_tone_fuzzy)

        # VisualizaciÃ³n principal
        st.subheader("Overall Tone Distribution")
        st.bar_chart(df['tone'].value_counts())

        st.subheader("Tone by User")
        tone_user = df.groupby(['user', 'tone']).size().unstack(fill_value=0)
        st.dataframe(tone_user.style.highlight_max(axis=1))

        st.subheader("Tone Evolution Over Time")
        tone_daily = df.groupby(['day', 'tone']).size().unstack(fill_value=0)
        st.area_chart(tone_daily)

        st.subheader("Top Tone per Day")
        if not tone_daily.empty:
            top_tone_day = tone_daily.idxmax(axis=1).value_counts()
            st.write(top_tone_day)

        st.subheader("Average Sentiment by User")
        st.bar_chart(df.groupby('user')['sentiment'].mean().sort_values())

        st.subheader("Tone Heatmap per User")
        import seaborn as sns
        import matplotlib.pyplot as plt
        fig, ax = plt.subplots(figsize=(10, 5))
        sns.heatmap(tone_user, cmap='YlOrRd', annot=True, fmt='d', ax=ax)
        st.pyplot(fig)

        st.subheader("Word Clouds per Tone")
        from wordcloud import WordCloud
        col1, col2 = st.columns(2)
        for i, tone in enumerate(tone_keywords):
            subset = df[df['tone'] == tone]
            if subset.empty:
                continue
            text = ' '.join(subset['message'].tolist())
            wc = WordCloud(width=400, height=200, background_color='white').generate(text)
            with (col1 if i % 2 == 0 else col2):
                st.markdown(f"**{tone.capitalize()}**")
                fig_wc, ax_wc = plt.subplots()
                ax_wc.imshow(wc, interpolation='bilinear')
                ax_wc.axis('off')
                st.pyplot(fig_wc)

        st.subheader("Top Days by Affection and Aggression")
        if 'affection' in tone_daily.columns:
            st.markdown("**Most Affectionate Days:**")
            st.write(tone_daily['affection'].sort_values(ascending=False).head(3))
        else:
            st.info("No affectionate messages found.")

        if 'aggressive' in tone_daily.columns:
            st.markdown("**Most Aggressive Days:**")
            st.write(tone_daily['aggressive'].sort_values(ascending=False).head(3))
        else:
            st.info("No aggressive messages found.")




#         # Palabras clave ampliadas en catalÃ¡n y espaÃ±ol para mÃºltiples categorÃ­as de tono/emociÃ³n
#         tono_palabras = {
#             'â¤ï¸ CariÃ±oso': [
#             'amor', 'tqm', 'beso', 'abrazo', 'cariÃ±o', 'te quiero', 'guapo', 'guapa', 'bonita', 'preciosa', 'mua',
#             't\'estimo', 'estimo', 'abraÃ§ada', 'petÃ³', 'mac@', 'preciosa', 'rei', 'reina', 'tq', 'molt amor',
#             'querido', 'querida', 'cari', 'precioso', 'preciosa', 'lindo', 'linda', 'hermoso', 'hermosa', 'encantador',
#             'encantadora', 'adoro', 'adorable', 'dulce', 'dulzura', 'cielo', 'corazÃ³n', 'mi vida', 'mi alma', 'tesoro',
#             'bonic', 'bonica', 'carinyo', 'carinyet', 'petonet', 'petonets', 'abraÃ§ades', 'tâ€™estimo molt', 'tâ€™estim',
#             'muac', 'muack', 'muacks', 'besitos', 'besote', 'besotes', 'abrazote', 'abrazotes', 'amorcito', 'amore',
#             'amig@', 'amiga', 'amigo', 'compaÃ±er@', 'compaÃ±era', 'compaÃ±ero', 'estimada', 'estimado', 'preci',
#             'guapetÃ³n', 'guapetona', 'bonÃ­ssim', 'bonÃ­ssima', 'encant', 'encantat', 'encantada', 'idol', 'idol@',
#             'idolito', 'idolita', 'adoraciÃ³n', 'adorad@', 'adorada', 'adorado', 'cariÃ±ito', 'cariÃ±osa', 'cariÃ±oso'
#             ],
#             'ğŸ˜¡ Agresivo': [
#             'odio', 'idiota', 'cÃ¡llate', 'pesado', 'estÃºpido', 'mierda', 'joder', 'gilipollas', 'tonto', 'calla',
#             'imbÃ¨cil', 'pesat', 'merda', 'capullo', 'estÃºpid', 'pallasso', 'collons', 'imbÃ©cil', 'asqueroso',
#             'asquerosa', 'maldito', 'maldita', 'cabron', 'cabrona', 'puta', 'puto', 'put@', 'putada', 'cojones',
#             'coÃ±o', 'hostia', 'hostias', 'malparit', 'malparida', 'malparido', 'subnormal', 'cretino', 'cretina',
#             'burro', 'burra', 'burro/a', 'tont@', 'tonta', 'tonto', 'tontorrÃ³n', 'tontorrona', 'payaso', 'payasa',
#             'pallasso', 'pallassa', 'gilipuertas', 'imbecil', 'imbÃ©cil', 'pesada', 'pesado', 'cansino', 'cansina',
#             'cansad@', 'cansada', 'cansado', 'plasta', 'plasta!'
#             ],
#             'ğŸ˜‚ Humor': [
#             'jaja', 'jeje', 'jajaja', 'lol', 'xd', 'xddd', 'risas', 'carcajada', 'carcajadas', 'gracioso', 'graciosa',
#             'chiste', 'broma', 'bromita', 'jijiji', 'juas', 'lolazo', 'humor', 'divertido', 'divertida', 'ğŸ¤£', 'ğŸ˜¹', 'ğŸ˜†', 'ğŸ˜„'
#             ],
#             'ğŸ˜¢ Triste': [
#             'triste', 'lloro', 'llorando', 'pena', 'deprimido', 'deprimida', 'depre', 'decepciÃ³n', 'decepcionado',
#             'decepcionada', 'desanimado', 'desanimada', 'lÃ¡grima', 'lÃ¡grimas', 'ploro', 'plorant', 'plorant', 'plorera',
#             'plor', 'trist', 'trista', 'tristesa', 'tristeza', 'ğŸ˜­', 'ğŸ˜¢', 'ğŸ˜', 'ğŸ˜”'
#             ],
#             'ğŸ˜± Sorpresa': [
#             'sorpresa', 'sorprendido', 'sorprendida', 'increÃ­ble', 'no me lo creo', 'alucino', 'flipante', 'wow',
#             'madre mÃ­a', 'impresionante', 'inesperado', 'inesperada', 'quÃ© fuerte', 'ostras', 'ostia', 'ostia!', 'ğŸ˜±', 'ğŸ˜²', 'ğŸ˜®'
#             ],
#             'ğŸ˜ Neutro': [
#             'ok', 'vale', 'bueno', 'bien', 'normal', 'asÃ­', 'pues', 'entonces', 'de acuerdo', 'okey', 'okeydokey', 'okis', 'okey', 'okey!', 'ok!', 'ğŸ‘Œ', 'ğŸ‘'
#             ],
#             'ğŸ˜… Nervioso': [
#             'uff', 'madre mÃ­a', 'ay', 'ayyy', 'ay dios', 'madre', 'madre mia', 'madre mÃ­a', 'nervioso', 'nerviosa', 'quÃ© nervios', 'ansioso', 'ansiosa', 'ansiedad', 'ğŸ˜…', 'ğŸ˜¬'
#             ],
#             'ğŸ˜‡ Agradecido': [
#             'gracias', 'grÃ cies', 'merci', 'thank you', 'agradecido', 'agradecida', 'te lo agradezco', 'mil gracias', 'muchas gracias', 'graciasss', 'graciass', 'gracias!', 'ğŸ™', 'ğŸ¤—'
#             ],
#             'ğŸ˜¤ FrustraciÃ³n': [
#             'uff', 'pfff', 'argh', 'quÃ© rabia', 'rabia', 'frustrado', 'frustrada', 'frustrante', 'me canso', 'cansado', 'cansada', 'cansancio', 'me molesta', 'molesto', 'molesta', 'ğŸ˜¤', 'ğŸ˜ '
#             ]
#         }

#         def clasificar_tono(msg):
#             msg_lower = msg.lower()
#             for tono, palabras in tono_palabras.items():
#                 for palabra in palabras:
#                     if palabra in msg_lower:
#                         return tono
#                     return 'ğŸ¤” Otro'

#         df['tono'] = df['message'].apply(clasificar_tono)

#         # Conteo general
#         st.subheader("DistribuciÃ³n general de tono")
#         st.bar_chart(df['tono'].value_counts())

#         # Por usuario
#         st.subheader("Ranking de tono por usuario")
#         tono_usuarios = df.groupby(['user', 'tono']).size().unstack(fill_value=0)
#         st.dataframe(tono_usuarios)

#         # Por dÃ­a
#         st.subheader("EvoluciÃ³n diaria de mensajes por tipo de tono")
#         tono_diario = df.groupby(['day', 'tono']).size().unstack(fill_value=0)
#         st.area_chart(tono_diario)

#         # Sentimiento medio por persona
#         st.subheader("Sentimiento medio por persona")
#         sent_por_usuario = df.groupby('user')['sentiment'].mean().sort_values()
#         st.bar_chart(sent_por_usuario)

#         # DÃ­a mÃ¡s cariÃ±oso o agresivo
#         st.subheader("DÃ­as con mÃ¡s mensajes cariÃ±osos o agresivos")
#         top_dias = df.groupby(['day', 'tono']).size().unstack(fill_value=0)
#         top_cariÃ±o = top_dias['â¤ï¸ CariÃ±oso'].sort_values(ascending=False).head(3)
#         top_enfado = top_dias['ğŸ˜¡ Agresivo'].sort_values(ascending=False).head(3)
#         st.markdown("**ğŸ¥° DÃ­as mÃ¡s cariÃ±osos:**")
#         st.write(top_cariÃ±o)
#         st.markdown("**ğŸ˜¤ DÃ­as mÃ¡s agresivos:**")
#         st.write(top_enfado)


else:
    st.info("Please upload a WhatsApp chat file to begin.")
